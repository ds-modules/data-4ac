{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqGJpQTYpDaM"
   },
   "source": [
    "# Notebook 5: Bias in Algorithmic Risk Scores for Health\n",
    "\n",
    "\n",
    "\n",
    "This lab is based off of [Dissecting racial bias in an algorithm used to manage the health of populations](https://science.sciencemag.org/content/366/6464/447) by Ziad Obermeyer et al (2019).\n",
    "\n",
    "# Table of Contents \n",
    "\n",
    "1. [What is Algorithmic Bias?](#0)<br>\n",
    "2. [Medical Cost and Risk](#1)<br>\n",
    "3. [Chronic Illness and Risk](#2)<br>\n",
    "4. [Interactions Between Cost and Illness](#3)<br>\n",
    "5. [Conclusions and Takeaways](#4)<br>\n",
    "6. [An Improved Algorithm?](#5)<br>\n",
    "7. [References](#6)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "B4AtgFvfIpjv",
    "outputId": "702842aa-d412-466d-eec5-b2240a779143"
   },
   "outputs": [],
   "source": [
    "# Run this cell! \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "%matplotlib inline\n",
    "\n",
    "import otter\n",
    "generator = otter.Notebook()\n",
    "\n",
    "from scripts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY-sa4O5IqCJ"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. What is Algorithmic Bias?<a id='0'></a>\n",
    "\n",
    "The term 'bias,\"has a number of meanings that depend upon the context in which it is used. When we describe a person or a situation to be biased, we usually mean that that person or situation favors one thing, person or group over another. Frequently, this favor or prejudice is deemed unfair, thus bias in common parlance can take on a negative weight. In statistics, bias (of an estimator) is defined as \"the difference between this estimator's expected value and the true value of the parameter being estimated.\" Recently, people have identified bias in algorithmic models. **Algorithmic bias** refers to algorithmic models that systematically and unfairly discriminate against certain individuals or groups in favor of others. The concept of \"algorithmic bias\" is provocative because people tend to think of algorithmic systems as \"neutral\" and \"objective.\" Researchers have discovered biases in algorithmic models that [produce search engine results](https://www.technologyreview.com/2018/02/26/3299/meet-the-woman-who-searches-out-search-engines-bias-against-women-and-minorities/), [underpin social media platforms](https://www.bbc.com/news/technology-53498685), [screen resumes](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G), determine mortgages, and [support health care decisions](https://www.nytimes.com/2020/08/25/sports/football/nfl-concussion-racial-bias.html).\n",
    "\n",
    "From [*Bias in Computer Systems* by Friedman and Nissenbaum](https://nissenbaum.tech.cornell.edu/papers/biasincomputers.pdf), bias arises in three primary ways in computer systems: \n",
    "- **Preexisting bias** arises when computer systems embody biases that exist independently of the system and usually prior to its creation, then perpetuates the preexisting bias. It can creep into a system through conscious and explicit efforts of individuals or institutions, or unconsciously despite good intentions.\n",
    "- **Technical bias** arises from system constraints or technical considerations, such as hardware limitation, random number generation, or even making human constructs usable by a computer (quantifying qualitative data for example).\n",
    "- **Emergent bias** emerges in the context of use after the system has been designed. \n",
    "\n",
    "--- \n",
    "\n",
    "## Background\n",
    "\n",
    "To effectively manage patients, health systems often need to estimate particular patients' health risks, or the likelihood that a patient will develop certain types of medical conditions in the future. Quantitative measures, or \"risk scores,\" can help health care providers to prioritize patients for certain kinds of treatments and preventive interventions, helping them to allocate resources to patients who need them.\n",
    "\n",
    "In this lab, we examine an algorithm widely-used in the health care industry to establish quantitative risk scores for patients. This serves as a screening tool to alert primary care doctors to high-risk patients. This algorithm uses medical cost (i.e., the amount that a patient's medical treatments cost) as a proxy for health. Through analysis of the data used in the model, we will discover how this algorithm embeds a bias against Black patients, undervaluing their medical risk relative to White patients: given the same risk score, Black patients are sicker than White patients. Crucially, this bias is not immediately visible when comparing medical costs across White and Black patients.\n",
    "\n",
    "We'll follow similar steps taken in the paper to uncover bias in this particular health risk algorithm by decomposing the variables in relation to the calculated risk scores. The key insight of Obermeyer et al's work is that bias frequently slips into algorithmic systems unnoticed, particularly when sensitive characteristics (such as race) are omitted or backgrounded in the data science process. Bias in algorithms affects people's lives very concretely: it makes it more difficult for Black patients to receive the care they need.\n",
    "\n",
    "-----\n",
    "## 2. Medical Cost and Risk <a id='1'></a>\n",
    "\n",
    "To help hospitals and insurance companies identify patients who should qualify for “high-risk care management” programs, the algorithm assigns each patient a risk score from 0 to 100. It predicts a patient’s total medical expenditure based on data from insurance claims (age, sex, diagnosis codes, etc.) and uses this variable as a proxy for health care needs.  Patient risk scores are then generated as functions of their predicted expenditures. For example, a patient with a calculated risk score of 80 has a predicted cost of 126,600 and a patient with a calculated risk score of .4 has a predicted cost of 500.\n",
    "\n",
    "If the model is calibrated (adjusted so the predictions match the empirical data) across race in terms of risk score and expenditure, Black and White patients with a given risk score should have similar total medical expenditures, on average.\n",
    "\n",
    "To see if this is true, we will generate a graph that shows the mean total medical expenditure by race given a risk score percentile.\n",
    "\n",
    "The first step is to load the data the model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "GAPNyZWblYpI",
    "outputId": "db4c7e3f-ed02-4744-9320-7950ff949cba"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('health-care-bias-lab.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add a column of risk percentiles to the data frame named `risk_percentile`. We create the column using the `convert_to_percentile` utility function. This function takes in a data frame and column name that we want to convert, then transformed the data in the specified column into percentiles. The transformed percentile data is then added as a new column to our data.\n",
    "\n",
    "The *risk percentile* of an individual is the value below which a percentage of the data (risk score) falls. This risk score is assigned by the algorithm to an individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "qRr4YHhrJs0X",
    "outputId": "a8f2305f-19a3-4e09-a8a8-cd3e3f335756"
   },
   "outputs": [],
   "source": [
    "# convert risk scores to perentiles\n",
    "risk_percentile = convert_to_percentile(data, \"risk_score_t\")\n",
    "\n",
    "# add a new column \"risk_percentile\"\n",
    "data[\"risk_percentile\"] = risk_percentile\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a data frame called `group_cost` with the average total medical expenditure for each race at each risk percentile. We do this by grouping the data by risk percentile and race, then taking the mean of `cost_t` for each group, which contains the average total cost associated with the specific risk percentage and race combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "r6PlT59kLq_F",
    "outputId": "ed746f98-c6ea-4f7a-b893-a90fd3e80dcd"
   },
   "outputs": [],
   "source": [
    "# grouping by \"risk_percentile\" and \"race\"\n",
    "group_cost = data.groupby([\"risk_percentile\", \"race\"])\n",
    "\n",
    "# taking the mean of each respective group in the data\n",
    "group_cost = group_cost[[\"cost_t\"]].mean().reset_index()\n",
    "group_cost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the `group_cost` data frame into two data frames based on race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2FmeKvoLKMT"
   },
   "outputs": [],
   "source": [
    "b_cost = group_cost[group_cost['race'] == 'black']\n",
    "w_cost = group_cost[group_cost['race'] == 'white']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot is made to display the risk percentile against the medical cost which is categorized by race; specifically by Black and White patients. \n",
    "\n",
    "**Aside:** Why should we scatter our data points in this situation rather than use line plots seen in notebooks 1 and 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "DE_PxzyEpxIQ",
    "outputId": "b6d8c47e-08b3-42ba-989a-b764026645b5"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = \"risk_percentile\", y = \"cost_t\", data = group_cost, hue = \"race\", marker = \"x\", legend = \"full\")\n",
    "plt.yscale('log')\n",
    "ax.set_yticks([1000, 3000, 8000, 20000, 60000])\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "plt.legend;\n",
    "sns.set(rc={'figure.figsize':(9,7)})\n",
    "plt.xlabel('Risk Percentile', size = 17)\n",
    "plt.ylabel('Total Medical Expenditure', size = 17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scatter plot above, the x's that represent Black and White patients are very interspersed, which makes it difficult to see any relationships present between the two groups. To fix this, we fit a LOWESS (Locally Weighted Scatterplot Smoothing) model to the data for each race. LOWESS models are used to create smooth lines through scatter plot data that help highlight relationships between variables and trends that might be present. The lines are created by computing the average data point using neighboring data points for each point on the x-axis. In this case, the LOWESS model calculated the average total medical expenditure for each risk percentile in black and white patients. \n",
    "\n",
    "The specifics of how LOWESS models are created is out of the scope of this notebook, but if you're interested in knowing more, you can read  about LOWESS models [here](https://blogs.sas.com/content/iml/2016/10/17/what-is-loess-regression.html#:~:text=Loess%20regression%20is%20a%20nonparametric,model%20with%20a%20parametric%20curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqoKswf-JV7K"
   },
   "outputs": [],
   "source": [
    "risk_percentile_array_b = np.array(b_cost['risk_percentile'])\n",
    "risk_percentile_array_w = np.array(w_cost['risk_percentile'])\n",
    "b_cost_array = np.array(b_cost['cost_t'])\n",
    "w_cost_array = np.farray(w_cost['cost_t'])\n",
    "b_cost_lowess = lowess(b_cost_array, risk_percentile_array_b, it=35, frac=0.2, delta=2)\n",
    "w_cost_lowess = lowess(w_cost_array, risk_percentile_array_w, it=35, frac=0.2, delta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we plot a curve fitted by the LOWESS model along with the data points in the plot from before, which results in the orange and blue lines through the graph. The orange and blue lines display the overall relationship of total medical expenditure and risk percentile in White and Black patients respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "GRoFxtbYyHI7",
    "outputId": "df576c50-3b92-4fd9-be74-cf13307cbc61"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = \"risk_percentile\", y = \"cost_t\", data = group_cost, hue = \"race\", marker = \"x\", legend = \"full\")\n",
    "plt.yscale('log')\n",
    "plt.plot(risk_percentile_array_b, b_cost_lowess[:, 1])\n",
    "plt.plot(risk_percentile_array_w, w_cost_lowess[:, 1])\n",
    "ax.set_yticks([1000, 3000, 8000, 20000, 60000])\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "plt.xlabel('Risk Percentile', size = 17)\n",
    "plt.ylabel('Total Medical Expenditure', size = 17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EHyhQ6HJXbf"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 2.1:</b> What do you notice about the relationship between medical expenditure and risk score by race? Can you conclude from this data that the model is biased?\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q21\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19CQRP0cJvU5"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "----\n",
    "\n",
    "## 3. Chronic Illness and Risk <a id='2'></a>\n",
    "\n",
    "Next, we will check to see if the model is calibrated across groups in terms of risk score and chronic illness. In other words, we will check to see whether Black and White patients have the same level of health for a given risk score.\n",
    "\n",
    "If cost is a good proxy for need, we would expect the shape of the curves for the two groups to be close to each other as it was in the prior graph.  In other words, health care cost conditional on health should not vary between groups.\n",
    "\n",
    "Similar to the previous process, we begin by creating a data frame with the average number of chronic illnesses for each race at each risk percentile. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "45SnmB-XLTEz",
    "outputId": "c8f7ce46-1214-484c-c4f4-1ab1da95e49a"
   },
   "outputs": [],
   "source": [
    "grouped_by_race = data.groupby([\"risk_percentile\", \"race\"])[[\"gagne_sum_t\"]].mean().reset_index()\n",
    "grouped_by_race.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we divide the grouped dataframe into two dataframes based on race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbYEyXWELZfA"
   },
   "outputs": [],
   "source": [
    "black_patients = grouped_by_race[grouped_by_race['race'] == 'black']\n",
    "white_patients = grouped_by_race[grouped_by_race['race'] == 'white']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a scatterplot of risk percentile against the average number of chronic ilnesses and categorize by race.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "W62DjFtgsGCM",
    "outputId": "aea8d426-820d-4e91-841f-7b653a470222"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=\"risk_percentile\", y=\"gagne_sum_t\", data=grouped_by_race, hue=\"race\", marker=\"x\", legend=\"full\");\n",
    "plt.xlabel('Risk Percentile', size = 17)\n",
    "plt.ylabel('Average Number of Chronic Illnesses', size = 17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the plot in the previous section, the trends of the scatter plot above are a bit more obvious. In this example, we fit a Generalized Linear Model (GLM) to the data for each race. Again, the specifics of how GLMs work is out of the scope of this notebook, but you can feel free to read more about GLMs [here](https://towardsdatascience.com/generalized-linear-models-9ec4dfe3dc3f). What's important for us to know about GLMs is that, similar to the LOWESS model, GLMs also create a smooth line through the data that helps to see trends in the data. This is beneficial because we want to identify any potential disparities between black and white patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "CKC07jBILlSN",
    "outputId": "0d28e75e-fba1-45c6-a6ce-cf73b6f1d89a"
   },
   "outputs": [],
   "source": [
    "X_b = sm.add_constant(np.array(black_patients[\"risk_percentile\"]))\n",
    "model_b = sm.GLM(list(black_patients[\"gagne_sum_t\"]), X_b, family=sm.families.Gaussian(sm.families.links.log()))\n",
    "model_b_results = model_b.fit()\n",
    "\n",
    "X_w = sm.add_constant(np.array(white_patients[\"risk_percentile\"]))\n",
    "model_w = sm.GLM(list(white_patients[\"gagne_sum_t\"]), X_w, family=sm.families.Gaussian(sm.families.links.log()))\n",
    "model_w_results = model_w.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the GLM model on the scatter plot. The orange and blue lines display the overall relationship between the average number of chronic illnesses and risk percentile in white and black patients respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "_3hWBonhJuTr",
    "outputId": "cf661bef-6a7a-49fd-d987-a88132ca2571"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"risk_percentile\", y=\"gagne_sum_t\", data=grouped_by_race, hue=\"race\", marker=\"x\", legend=\"full\")\n",
    "plt.plot(np.array(black_patients[\"risk_percentile\"]), model_b_results.predict())\n",
    "plt.plot(np.array(white_patients[\"risk_percentile\"]), model_w_results.predict())\n",
    "plt.legend();\n",
    "plt.xlabel('Risk Percentile', size = 17)\n",
    "plt.ylabel('Average Number of Chronic Illnesses', size = 17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIEeHKYUKb-j"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 3.1:</b> What do you notice about the relationship between chronic illness and risk score by race? What, if anything, does the data analysis we have done tell us about whether the model is racially biased or not?\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q31\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZhqmFQOKuxi"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "----\n",
    "## 4. Interactions Between Cost and Illness <a id='3'></a>\n",
    "\n",
    "Our work above shows us that a black patient and a white patient with the same risk score tend to spend the same amount on medical care on average, yet the Black patient tends to have more chronic illnesses.\n",
    "\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 4.1:</b> Why might this be the case? What barriers, injustices, or systemic issues might black patients face or have faced that could be causing this disparity?\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q41\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "--- \n",
    "To understand this interaction, we will generate a graph that shows the mean total medical expenditure by race, given the number of chronic illnesses.\n",
    "\n",
    "First, we add a column of illness percentiles to the data frame called `illness_percentile`. The illness percentile is found using the convert_to_percentile utility function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "OootdJVKLyCx",
    "outputId": "51358d52-c0e9-40c0-fe18-3d1b882472aa"
   },
   "outputs": [],
   "source": [
    "illness_percentile = convert_to_percentile(data, \"gagne_sum_t\")\n",
    "data['illness_percentile'] = illness_percentile\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a data frame which has the average total medical expenditure for each race (black and white patients) at each illness percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "RYBDYKD_uaSq",
    "outputId": "712eb75c-4dbf-49b8-88f8-8a29d9769582"
   },
   "outputs": [],
   "source": [
    "illnesses = data.groupby([\"illness_percentile\", \"race\"])[[\"cost_t\"]].mean().reset_index()\n",
    "illnesses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the illnesses into two data frames based on race; specifically black and white patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arGPABGoL1hV"
   },
   "outputs": [],
   "source": [
    "illness_b = illnesses[illnesses['race'] == 'black']\n",
    "illness_w = illnesses[illnesses['race'] == 'white']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the process in Section 1, we create a scatterplot of illness percentile against the medical cost and group by race, and then fit a LOWESS model to the scaterplot for each race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GE1_aZaMxy_"
   },
   "outputs": [],
   "source": [
    "illness_percentile_array_b = np.array(illness_b['illness_percentile'])\n",
    "illness_percentile_array_w = np.array(illness_w['illness_percentile'])\n",
    "illness_cost_b = np.array(illness_b['cost_t'])\n",
    "illness_cost_w = np.array(illness_w['cost_t'])\n",
    "b_illness_lowess = lowess(illness_cost_b, illness_percentile_array_b, it=35, frac=0.3, delta=2)\n",
    "w_illness_lowess = lowess(illness_cost_w, illness_percentile_array_w, it=35, frac=0.3, delta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the LOWESS model on the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "v_lLZlMvybtB",
    "outputId": "29089084-83ab-48b4-edfa-d8945dca9e38"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = \"illness_percentile\", y = \"cost_t\", data = illnesses, hue = \"race\", marker = \"x\", legend = \"full\")\n",
    "plt.yscale('log')\n",
    "plt.plot(illness_percentile_array_b, b_illness_lowess[:, 1])\n",
    "plt.plot(illness_percentile_array_w, w_illness_lowess[:, 1])\n",
    "ax.set_yticks([1000, 3000, 8000, 20000, 60000])\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "plt.xlabel('Illness Percentile', size = 17)\n",
    "plt.ylabel('Total Medical Expenditure', size = 17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWVBYMvMNRei"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 4.2:</b> What can you conclude about the relationship between cost and chronic illness? Why might this relationship exist? What are consequences for the risk score model?\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q42\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esq4RAPfOnro"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusions and Takeaways <a id='4'></a>\n",
    "\n",
    "Even systems that appear balanced across racial groups at first glance may belie underlying biases in the datasets. Thus, seemingly unbiased predictors, such as cost, can in fact be highly correlated with a biasing variable such as race, gender, income or other relational characteristics.\n",
    "\n",
    "In this example, bias emerged from using an indicator of need (cost) that was itself influenced by race. Biased estimation of need between races resulted.\n",
    "\n",
    "To better understand the ways in which race influences health care cost, here is a segment from Obermeyer et al’s paper:\n",
    " \n",
    ">The literature broadly suggests two main potential channels [through which racial discrepancies in healthcare cost are created]. **First, poor patients face substantial barriers to accessing health care, even when enrolled in insurance plans.** Although the population we study is entirely insured, there are many other mechanisms by which poverty can lead to disparities in use of health care: geography and differential access to transportation, competing demands from jobs or child care, or knowledge of reasons to seek care (1-3). To the extent that race and socioeconomic status are correlated, these factors will differentially affect Black patients. **Second, race could affect costs directly via several channels: direct (“taste-based”) discrimination, changes to the doctor–patient relationship, or others.** A recent trial randomly assigned Black patients to a Black or White primary care provider and found significantly higher uptake of recommended preventive care when the provider was Black (4). This is perhaps the most rigorous demonstration of this effect, and it fits with a larger literature on potential mechanisms by which race can affect health care directly. For example, it has long been documented that Black patients have reduced trust in the health care system (5), a fact that some studies trace to the revelations of the Tuskegee study and other adverse experiences (6). A substantial literature in psychology has documented physicians’ differential perceptions of Black patients, in terms of intelligence, affiliation (7), or pain tolerance (8). **Thus, whether it is communication, trust, or bias, something about the interactions of Black patients with the health care system itself leads to reduced use of health care. The collective effect of these many channels is to lower health spending substantially for Black patients, conditional on need—a finding that has been appreciated for at least two decades (9, emphasis added).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeHiQ6fz3nuC"
   },
   "source": [
    "----\n",
    "\n",
    "## 6. An Improved Algorithm? <a id='5'></a>\n",
    "\n",
    "Following up with the previous section, how could we use the data we have to create new proxies for health needs that may be less biased than medical costs? Obermeyer et al. suggested to keep the model infrastructure - the sample (data used), predictors (excluding race), training process, etc. - but changing the label. Instead of future cost, they created a variable that combined health prediction with cost prediction which resulted in an 84% reduction in bias. \n",
    "\n",
    "Though the labels of the data collected reflect structural inequalities, the results suggest that biases are \"fixable\" and call for changing the data we feed algorithms, specifically the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 6.1:</b> Suppose a less biased algorithm is made available to health care systems. To what extent is a technical fix (such as using less biased predictors) helpful? What kinds of changes need to be implemented in the health care system?\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q61\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 6.2:</b> What are other applications of prediction algorithms where this type of bias may also arise?\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q62\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Labor Question:</b> How much time did you spend completing this module? Did you find outside resources that helped you? if so what were they?\n",
    "</div>\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: QL\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** *Your Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Feedback Survey\n",
    "Please consider filling out this [survey](https://docs.google.com/forms/d/e/1FAIpQLScd1q8VqvOMuVvLfhbVswckYKg1HFVwVu_bTF5NWbVZr4qWhw/viewform?usp=sf_link) to help us improve this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your notebook first, then go to `Kernel` > `Restart and Run All Cells`\n",
    "# Download the zip file, which contains a copy of your notebook and your written responses.\n",
    "generator.export(\"notebook5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpnQ5tGH4A-9"
   },
   "source": [
    "----\n",
    "\n",
    "## 7. References <a id='6'></a>\n",
    "\n",
    "Adapted from:\n",
    "ML Failures lab: Dissecting Racial Bias by Nick Merrill, Inderpal Kaur, Samuel Greenberg is licensed under CC BY-NC-SA 4.0. To view a copy of this license, visit https://creativecommons.org/licenses/by-nc-sa/4.0\n",
    "\n",
    "1. K. Fiscella, P. Franks, M. R. Gold, C. M. Clancy, JAMA 283, 2579–2584 (2000).\n",
    "2. N. E. Adler, K. Newman, Health Aff. 21, 60–76 (2002).\n",
    "3. N. E. Adler, W. T. Boyce, M. A. Chesney, S. Folkman, S. L. Syme, JAMA 269, 3140–3145 (1993).\n",
    "4. M. Alsan, O. Garrick, G. C. Graziani, “Does diversity matter for health? Experimental evidence from Oakland” (National Bureau of Economic Research, 2018).\n",
    "5. K. Armstrong, K. L. Ravenell, S. McMurphy, M. Putt, Am. J. Public Health 97, 1283–1289 (2007).\n",
    "6. M. Alsan, M. Wanamaker, Q. J. Econ. 133, 407–455 (2018).\n",
    "7. M. van Ryn, J. Burke, Soc. Sci. Med. 50, 813–828 (2000).\n",
    "8. K. M. Hoffman, S. Trawalter, J. R. Axt, M. N. Oliver, Proc. Natl. Acad. Sci. U.S.A. 113, 4296–4301 (2016).\n",
    "9. J. J. Escarce, F. W. Puffer, in Racial and Ethnic Differences in the Health of Older Americans (National Academies Press, 1997), chap. 6; www.ncbi.nlm.nih.gov/books/ NBK109841/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Health_care_lab_for_review-text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
